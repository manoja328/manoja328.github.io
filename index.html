<html>
<head>
<link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
<link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,500,700|Crete+Round" rel="stylesheet" type="text/css">
<link rel="stylesheet" href="style.css">
<script src="hidebib.js" type="text/javascript"></script>
<link href="academicons.css" rel="stylesheet">    
<meta charset="UTF-8">
</head>
<body>
<title>Manoj Acharya</title>

<div>
    <div class="txt-panel">
        <h1>Manoj Acharya</h1>
        <p>Machine Learning Researcher at SRI International</p>
<!--         <p>Email: manoj DOT acharya AT sri DOT com ; ma7583 AT rit DOT edu, manoja328 AT gmail DOT com</p> -->
<!--         <a href="https://www.linkedin.com/in/ermanojacharya">LinkedIn</a> -->
<!--         <a href="https://github.com/manoja328">GitHub</a><br><br> -->
<!-- <i class="fa fa-fw fa-envelope"></i> Email: manoj.acharya@sri.com, ma7583@rit.edu, manoja328@gmail.com <br>   -->
<i class="fa fa-fw fa-envelope"></i> Email: manoj.acharya@sri.com, ma7583@rit.edu<br>  
<a href="https://www.manojacharya.com/assets/Manoj_Acharya_cv.pdf"><i class="fa fa-fw fa-file"></i> CV</a><br>
<a href="https://twitter.com/manoja328"><i class="fa fa-fw fa-twitter-square"></i> Twitter</a><br>
<a href="https://github.com/manoja328"><i class="fa fa-fw fa-github"></i> Github</a><br>
<a href="https://www.linkedin.com/in/manojacharyarit"><i class="fa fa-fw fa-linkedin"></i> Linkedin</a><br>      
    </div> 
    <div class="img-panel">
        <img class="squarepic" src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/profile-pic.jpeg" />
    </div>
    <hr />
</div>    
    
<div class='nav-bar'>
<a href="/blog">Blog</a> /
<a href="http://www.cis.rit.edu/~ma7583/">Projects</a>
<!-- <a href="#research">Research Interests</a> | -->
<!-- <a href="#publications">Publications</a> |  -->
<!-- <a href="/assets/cv.pdf" target="_blank">CV</a> -->

</div>
    
<h2>Research Interests</h2>
    
I received my PhD from the Rochester Institute of Technology in 2022 where I worked with my adviser Dr. <a href="http://chriskanan.com" target="_blank"> Christopher Kanan </a>.
I enjoy doing research on Deep Learning especially on topics such as multimodal learning with language and vision(V&L), Object detection, Large Language Models, and Continual Learning.
Ideally, I would want to develop multimodal systems that can learn from large uncurated data and are robust enough to operate in an Open-World via continual knowledge exploration
and adaptation just like humans.

<h2>Timeline Events</h2>
<strong>June 2023:</strong> I joined <a href="https://www.sri.com/"> SRI International </a> as an Advanced Computer Scientist in the <a href="https://nusci.csl.sri.com/people/">  Neuro-symbolic Computing and Intelligence (NuSCI)</a> research group. <br>  
<strong>July 2022:</strong> I joined Amazon as an Applied Scientist in the Search team to help Amazon build foundation models. </a> <br>    
<strong>June 2022:</strong> Defended my dissertaion titled "Towards Multimodal Open-World Learning in Deep Neural Networks". <a href="https://scholarworks.rit.edu/theses/11233/"> Online link </a> <br>    
<strong>April 2022:</strong>Our paper "Detecting out-of-context objects using graph contextual reasoning network" is accepted to IJCAI-ECAI 2022 !!<br>    
<strong>Oct 2021:</strong> Secured second position in the SODA10M <a href="https://sslad2021.github.io/pages/challenge.html"> Continual Object Detection Challenge</a> at ICCV 2021 !!<br>
<strong>Sep 2021:</strong> Spent three months working as a Summer Research Intern at SRI International. <br>
<strong>April 2021:</strong> I successfully defended my dissertation proposal and advanced to candidacy. <br>
<strong>Jul 2020:</strong> Our paper "RODEO: Replay for Online Object Detection" is accepted to BMVC 2020!! <br>
<strong>Jul 2020:</strong> Our paper "REMIND Your Neural Network to Prevent Catastrophic Forgetting" is accepted to ECCV 2020! (27.1% acceptance rate) <br>
<strong>Oct 2019:</strong> Our work got featured in <a href="https://www.rit.edu/news/rit-researchers-win-first-place-international-eye-tracking-challenge-facebook-research"> RIT news</a>!!<br>
<strong>Sep 2019:</strong> We won the <a href="https://research.fb.com/programs/openeds-challenge/#Announcement_of_the_Challenge_Winners">Facebook Eye Tracking Semantic Segmentation </a> Challenge!!<br>
<strong>Feb 2019:</strong> Our short paper is accepted to NAACL 2019!! <br>
<strong>Nov 2018:</strong> TallyQA won the <b>best poster award</b> in the annual RIT Graduate Showcase!! <br>
<strong>Nov 2018:</strong> Our paper is accepted to AAAI 2019. (acceptance rate ~16%) <br>
<strong>July 2017:</strong> Started working at <a href="http://klab.cis.rit.edu/"> Klab </a> under Dr. Christopher Kanan.<br>  
<strong>July 2017:</strong> Passed the Imaging science PhD qualifying exam.    
 
<h2>Publications</h2>
For a complete and the most current list please refer to my <a href="https://scholar.google.com/citations?hl=en&user=dpQtmqQAAAAJ&view_op=list_works&sortby=pubdate"> Google Scholar profile</a>.

<table cellpadding="10" width="100%" align="center" border="0">
    <tbody>
        

        <tr>
          <td width="30%" valign="top">
            <img src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/assets/gcrncpic.png" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/2202.05930"> Detecting out-of-context objects using graph contextual reasoning network.</a><br>
            <strong>Manoj Acharya</strong>, Anirban Roy, Kaushik Koneripalli, Susmit Jha, Christopher Kanan, Ajay Divakaran<br>
            IJCAI-ECAI 2022 <br>
            <div class="paper" id="gcrn">
                <a href="javascript:toggleblock('gcrn_abs')">abstract</a> / 
                <a href="javascript:toggleblock('gcrn_bib')">bibtex</a> / 
                <a href="">code</a>
                <a href="https://www.ijcai.org/proceedings/2022/89">video</a>
                <p align="justify">
                    <i id="gcrn_abs" style="display: none;">This paper presents an approach to detect out-of-context (OOC) objects in an image. Given an image with a set of objects, our goal is to determine if an object is inconsistent with the scene context and detect the OOC object with a bounding box. In this work, we consider commonly explored contextual relations such as co-occurrence relations, the relative size of an object with respect to other objects, and the position of the object in the scene. We posit that contextual cues are useful to determine object labels for in-context objects and inconsistent context cues are detrimental to determining object labels for out-of-context objects. To realize this hypothesis, we propose a graph contextual reasoning network (GCRN) to detect OOC objects. GCRN consists of two separate graphs to predict object labels based on the contextual cues in the image: 1) a representation graph to learn object features based on the neighboring objects and 2) a context graph to explicitly capture contextual cues from the neighboring objects. GCRN explicitly captures the contextual cues to improve the detection of in-context objects and identify objects that violate contextual relations. In order to evaluate our approach, we create a large-scale dataset by adding OOC object instances to the COCO images. We also evaluate on recent OCD benchmark. Our results show that GCRN outperforms competitive baselines in detecting OOC objects and correctly detecting in-context objects.</i>
                </p>
                <bibtext id="gcrn_bib" style="display: none;">
                    <p>
                        @article{acharya2022detecting,
                          title={Detecting out-of-context objects using contextual cues},
                          author={Acharya, Manoj and Roy, Anirban and Koneripalli, Kaushik and Jha, Susmit and Kanan, Christopher and Divakaran, Ajay},
                          journal={arXiv preprint arXiv:2202.05930},
                          year={2022}
                        }
                        </p>
                </bibtext>
            </div>
        </td>
    </tr>        
                
        
    <tr>
          <td width="30%" valign="top">
            <img src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/assets/sodapic.jpg" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/2110.13064"> 2nd Place Solution for SODA10M Challenge 2021 -- Continual Detection Track.</a><br>
            <strong>Manoj Acharya</strong>, and Christopher Kanan<br>
            ICCVW 2021 <br>
            <div class="paper" id="soda">
                <a href="javascript:toggleblock('soda_abs')">abstract</a> / 
                <a href="javascript:toggleblock('soda_bib')">bibtex</a> / 
                <p align="justify">
                    <i id="soda_abs" style="display: none;">In this technical report, we present our approaches for the continual object detection track of the SODA10M challenge. We adapt ResNet50-FPN as the baseline and try several improvements for the final submission model. We find that task-specific replay scheme, learning rate scheduling, model calibration, and using original image scale helps to improve performance for both large and small objects in images. Our team `hypertune28' secured the second position among 52 participants in the challenge. This work will be presented at the ICCV 2021 Workshop on Self-supervised Learning for Next-Generation Industry-level Autonomous Driving (SSLAD).</i>
                </p>
                <bibtext id="soda_bib" style="display: none;">
                    <p>
                        @article{acharya20212nd,
                          title={2nd Place Solution for SODA10M Challenge 2021--Continual Detection Track},
                          author={Acharya, Manoj and Kanan, Christopher},
                          journal={arXiv preprint arXiv:2110.13064},
                          year={2021}
                        }
                        </p>
                </bibtext>
            </div>
        </td>
    </tr>
        
               
        
        <tr>
          <td width="30%" valign="top">
            <img src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/assets/rodeo.jpg" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/2008.06439">RODEO: Replay for Online Object Detection.</a><br>
            <strong>Manoj Acharya</strong>, Tyler L. Hayes, and Christopher Kanan<br>
            BMVC 2020 <br>
            <div class="paper" id="rodeo">
                <a href="javascript:toggleblock('rodeo_abs')">abstract</a> / 
                <a href="javascript:toggleblock('rodeo_bib')">bibtex</a> /
                <a href="https://github.com/manoja328/rodeo">code</a> /
                <a href="https://www.youtube.com/watch?v=DHzpb4oiKfA">video</a>
                <p align="justify">
                    <i id="rodeo_abs" style="display: none;">
                    Humans can incrementally learn to do new visual detection tasks, which is a huge challenge for today's computer vision systems. Incrementally trained deep learning models lack backwards transfer to previously seen classes and suffer from a phenomenon known as ``catastrophic forgetting.'' In this paper, we pioneer online streaming learning for object detection, where an agent must learn examples one at a time with severe memory and computational constraints. In object detection, a system must output all bounding boxes for an image with the correct label. Unlike earlier work, the system described in this paper can learn how to do this task in an online manner with new classes being introduced over time. We achieve this capability by using  a novel memory replay mechanism that replays entire scenes in an efficient manner. We achieve state-of-the-art results on both the PASCAL VOC 2007 and MS COCO datasets.
                    </i>
                </p>
                <bibtext id="rodeo_bib" style="display: none;">
                    <p>
                        @inproceedings{acharya2020rodeo,<br>
                          title={RODEO: Replay for Online Object Detection},<br>
                          author={Acharya, Manoj and Hayes, Tyler L. and Kanan, Christopher},<br>
                          booktitle={The British Machine Vision Conference},<br>
                          year={2020}<br>
                        }
                        </p>
                </bibtext>
            </div>
        </td>
    </tr>        
        
        
        
        
        <tr>
          <td width="30%" valign="top">
            <img src="https://github.com/tyler-hayes/REMIND/raw/master/repo_images/REMIND_model.png" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/1910.02509"> REMIND Your Neural Network to Prevent Catastrophic Forgetting.</a><br>
            Tyler L. Hayes*, Kushal Kafle*, Robik Shrestha*, <strong>Manoj Acharya</strong>, and Christopher Kanan<br>
            ECCV 2020 <br>
            <div class="paper" id="remind">
                <a href="javascript:toggleblock('remind_abs')">abstract</a> / 
                <a href="javascript:toggleblock('remind_bib')">bibtex</a> / 
                <a href="https://github.com/tyler-hayes/REMIND">code</a>
                <p align="justify">
                    <i id="remind_abs" style="display: none;">People learn throughout life. However, incrementally updating conventional neural networks leads to catastrophic forgetting. A common remedy is replay, which is inspired by how the brain consolidates memory. Replay involves fine-tuning a network on a mixture of new and old instances. While there is neuroscientific evidence that the brain replays compressed memories, existing methods for convolutional networks replay raw images. Here, we propose REMIND, a brain-inspired approach that enables efficient replay with compressed representations. REMIND is trained in an online manner, meaning it learns one example at a time, which is closer to how humans learn. Under the same constraints, REMIND outperforms other methods for incremental class learning on the ImageNet ILSVRC-2012 dataset. We probe REMIND's robustness to data ordering schemes known to induce catastrophic forgetting. We demonstrate REMIND's generality by pioneering online learning for Visual Question Answering (VQA).</i>
                </p>
                <bibtext id="remind_bib" style="display: none;">
                    <p>
                        @article{hayes2019remind,<br>
                          title={REMIND Your Neural Network to Prevent Catastrophic Forgetting},<br>
                          author={Hayes, Tyler L. and Kafle, Kushal and Shrestha, Robik and Acharya, Manoj and Kanan, Christopher},<br>
                          journal={ECCV},<br>
                          year={2019} <br>
                        }
                        </p>
                </bibtext>
            </div>
        </td>
    </tr>




<tr>
          <td width="30%" valign="top">
            <img src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/assets/ritnet.png" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/1910.00694v1"> RITnet: Real-time Semantic Segmentation of the Eye for Gaze Tracking.</a><br>
            Aayush Chaudhary*, Rakshit Kothari*,<strong>Manoj Acharya*</strong>, Shusil Dangi, Nitinraj Nair, Reynold Bailey, Christopher Kanan ,Gabriel Diaz, Jeff Pelz<br>
            ICCVW 2019 <a style="color:red">(Competition Winner)</a> <br>
            <div class="paper" id="tallyqa">
                <a href="javascript:toggleblock('ritnet_abs')">abstract</a> / 
                <a href="javascript:toggleblock('ritnet_bib')">bibtex</a> / 
                <a href="https://bitbucket.org/eye-ush/ritnet/">code</a> 
                <p align="justify"><i id="ritnet_abs" style="display: none;">Accurate eye segmentation can improve eye-gaze estimation and support interactive computing based on visual attention; however, existing eye segmentation methods suffer from issues such as person-dependent accuracy, lack of robustness, and an inability to be run in real-time. Here, we present the RITnet model, which is a deep neural network that combines U-Net and DenseNet. RITnet is under 1 MB and achieves 95.3% accuracy on the 2019 OpenEDS Semantic Segmentation challenge. Using a GeForce GTX 1080 Ti, RITnet tracks at > 300Hz, enabling real-time gaze tracking applications. Pre-trained models and source code are available this <a href="https://bitbucket.org/eye-ush/ritnet/">https URL.</a></i>
                </p>
                <bibtext id="ritnet_bib" style="display: none;">
                    <p>
                       @inproceedings{chaudhary2019ritnet,<br>
                          title={RITnet: real-time semantic segmentation of the eye for gaze tracking},<br>
                          author={Chaudhary, Aayush K and Kothari, Rakshit and Acharya, Manoj and Dangi, Shusil and Nair, Nitinraj and Bailey, Reynold and Kanan, Christopher and Diaz, Gabriel and Pelz, Jeff B},<br>
                          booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},<br>
                          pages={3698--3702},<br>
                          year={2019},<br>
                          organization={IEEE} <br>
                        }
                    </p>
                </bibtext>
            </div>
        </td>
    </tr>

   

<tr>
          <td width="30%" valign="top">
            <img src="https://raw.githubusercontent.com/manoja328/manoja328.github.io/master/assets/vqd.png" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/1904.02794">VQD: Visual Query Detection in Natural Scenes.</a><br>
            <strong>Manoj Acharya</strong> , Karan Jariwala, Christopher Kanan<br>
            NAACL 2019<br>
            <div class="paper" id="tallyqa">
                <a href="javascript:toggleblock('vqd_abs')">abstract</a> / 
                <a href="javascript:toggleblock('vqd_bib')">bibtex</a> / 
                <a href="https://www.manojacharya.com/vqd/"> website</a>
                <p align="justify">
                    <i id="vqd_abs" style="display: none;">We propose Visual Query Detection (VQD), a new visual grounding task. In VQD, a system is guided by natural language to localize a variable number of objects in an image. VQD is related to visual referring expression recognition, where the task is to localize only one object. We describe the first dataset for VQD and we propose baseline algorithms that demonstrate the difficulty of the task compared to referring expression recognition.</i>
                </p>
                <bibtext id="vqd_bib" style="display: none;">
                    <p>
                        @article{acharya2019vqd,<br>
                          title={VQD: Visual query detection in natural scenes},<br>
                          author={Acharya, Manoj and Jariwala, Karan and Kanan, Christopher},<br>
                          journal={NAACL},<br>
                          year={2019} <br>
                        }
                    </p>
                </bibtext>
            </div>
        </td>
    </tr>

    <tr>
          <td width="30%" valign="top">
            <img src="https://kushalkafle.com/images/tallyqa.png" style="border-style: none" width="100%">
          </td>
          <td width="70%" valign="top">
            <a href="https://arxiv.org/abs/1810.12440">TallyQA: Answering Complex Counting Questions.</a><br>
            <strong>Manoj Acharya</strong> , Kushal Kafle, Christopher Kanan<br>
            AAAI 2019 <a style="color:red">(Spotlight Presentation)</a> <br>
            <div class="paper" id="tallyqa">
                <a href="javascript:toggleblock('tallyqa_abs')">abstract</a> / 
                <a href="javascript:toggleblock('tallyqa_bib')">bibtex</a> / 
                <a href="tallyqa.html"> website</a> / 
                <a href="https://github.com/manoja328/tallyqacode">code</a>
                <p align="justify">
                    <i id="tallyqa_abs" style="display: none;">Most counting questions in visual question answering (VQA) datasets are simple and require no more than object detection. Here, we study algorithms for complex counting questions that involve relationships between objects, attribute identification, reasoning, and more. To do this, we created TallyQA, the world's largest dataset for open-ended counting. We propose a new algorithm for counting that uses relation networks with region proposals. Our method lets relation networks be efficiently used with high-resolution imagery. It yields state-of-the-art results compared to baseline and recent systems on both TallyQA and the HowMany-QA benchmark.</i>
                </p>
                <bibtext id="tallyqa_bib" style="display: none;">
                    <p>
                        @inproceedings{acharya2019tallyqa,<br>
                          title={TallyQA: Answering complex counting questions},<br>
                          author={Acharya, Manoj and Kafle, Kushal and Kanan, Christopher},<br>
                          booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},<br>
                          volume={33},<br>
                          pages={8076--8084},<br>
                          year={2019} <br>
                        }
                    </p>
                </bibtext>
            </div>
        </td>
    </tr>
        
</tbody>
</table>


<div style="display: flex; align-items: center;">
  <h3 style="margin: 0;">Page Visits:</h3>
  <img src="https://hitwebcounter.com/counter/counter.php?page=16898392&style=0010&nbdigits=5&type=page" title="Counter Widget" alt="Visit counter For Websites" border="0" style="margin-left: 10px;" />
</div>
     
    
</body>
</html>
    



